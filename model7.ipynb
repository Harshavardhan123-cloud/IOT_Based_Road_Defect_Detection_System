{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO  # YOLOv5/YOLOv8 integration\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yolov12s.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m data_yaml_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(working_dir, \u001b[39m'\u001b[39m\u001b[39mDataset/data.yaml\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Load YOLO model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39;49m\u001b[39myolov12s.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Train with updated hyperparameters and Adam optimizer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     data\u001b[39m=\u001b[39mdata_yaml_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,  \u001b[39m# More epochs for better generalization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m     mixup\u001b[39m=\u001b[39mhyperparameters[\u001b[39m\"\u001b[39m\u001b[39mmixup\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.87.59.149/home/mt23mcs002/Downloads/Augmented_Images_Python/model7.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/ultralytics/models/yolo/model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39m=\u001b[39m new_instance\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[39m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model\u001b[39m=\u001b[39;49mmodel, task\u001b[39m=\u001b[39;49mtask, verbose\u001b[39m=\u001b[39;49mverbose)\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/ultralytics/engine/model.py:146\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(model, task\u001b[39m=\u001b[39mtask, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m    145\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load(model, task\u001b[39m=\u001b[39;49mtask)\n\u001b[1;32m    148\u001b[0m \u001b[39m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/ultralytics/engine/model.py:289\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    286\u001b[0m weights \u001b[39m=\u001b[39m checks\u001b[39m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[39m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m Path(weights)\u001b[39m.\u001b[39msuffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt \u001b[39m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[1;32m    290\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs[\u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    291\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moverrides \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_ckpt_args(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39margs)\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:900\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mattempt_load_one_weight\u001b[39m(weight, device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fuse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    899\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m     ckpt, weight \u001b[39m=\u001b[39m torch_safe_load(weight)  \u001b[39m# load ckpt\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     args \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mDEFAULT_CFG_DICT, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(ckpt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtrain_args\u001b[39m\u001b[39m\"\u001b[39m, {}))}  \u001b[39m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    902\u001b[0m     model \u001b[39m=\u001b[39m (ckpt\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mema\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m ckpt[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()  \u001b[39m# FP32 model\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/ultralytics/nn/tasks.py:827\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight, safe_only)\u001b[0m\n\u001b[1;32m    825\u001b[0m                 ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(f, pickle_module\u001b[39m=\u001b[39msafe_pickle)\n\u001b[1;32m    826\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m             ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(file, map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    829\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    830\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/ultralytics/utils/patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m TORCH_1_13 \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mweights_only\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mweights_only\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m _torch_load(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    752\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Downloads/Augmented_Images_Python/.venv/lib/python3.9/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov12s.pt'"
     ]
    }
   ],
   "source": [
    "# Define YOLO training parameters\n",
    "hyperparameters = {\n",
    "    \"lr0\": 0.001,  # Initial learning rate\n",
    "    \"lrf\": 0.2,  # Final learning rate fraction\n",
    "    \"momentum\": 0.937,  # Momentum\n",
    "    \"weight_decay\": 0.0005,  # L2 Regularization\n",
    "    \"warmup_epochs\": 3.0,  # Warm-up period\n",
    "    \"warmup_momentum\": 0.8,  # Warm-up momentum\n",
    "    \"warmup_bias_lr\": 0.1,  # Warm-up bias learning rate\n",
    "    \"box\": 0.05,  # Box loss gain\n",
    "    \"cls\": 0.5,  # Class loss gain\n",
    "    \"cls_pw\": 1.0,  # Class label smoothing\n",
    "    \"iou_t\": 0.2,  # IoU threshold for training\n",
    "    \"anchor_t\": 4.0,  # Anchor threshold\n",
    "    \"fl_gamma\": 0.0,  # Focal loss gamma\n",
    "    \"hsv_h\": 0.015,  # Hue augmentation\n",
    "    \"hsv_s\": 0.7,  # Saturation augmentation\n",
    "    \"hsv_v\": 0.4,  # Value augmentation\n",
    "    \"degrees\": 0.0,  # Rotation augmentation\n",
    "    \"translate\": 0.1,  # Translation augmentation\n",
    "    \"scale\": 0.5,  # Scale augmentation\n",
    "    \"shear\": 0.0,  # Shear augmentation\n",
    "    \"perspective\": 0.0,  # Perspective augmentation\n",
    "    \"flipud\": 0.0,  # Vertical flip probability\n",
    "    \"fliplr\": 0.5,  # Horizontal flip probability\n",
    "    \"mosaic\": 1.0,  # Mosaic augmentation\n",
    "    \"mixup\": 0.2,  # Mixup augmentation\n",
    "}\n",
    "\n",
    "# Paths\n",
    "working_dir = os.getcwd()\n",
    "data_yaml_path = os.path.join(working_dir, 'Dataset/data.yaml')\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO('yolov12s.pt')\n",
    "\n",
    "# Train with updated hyperparameters and Adam optimizer\n",
    "results = model.train(\n",
    "    data=data_yaml_path,\n",
    "    epochs=30,  # More epochs for better generalization\n",
    "    imgsz=640,\n",
    "    project='road_defect_yolo',\n",
    "    name='experiment3',\n",
    "    exist_ok=True,\n",
    "    optimizer='Adam',  # Use Adam optimizer\n",
    "    lr0=hyperparameters[\"lr0\"],\n",
    "    lrf=hyperparameters[\"lrf\"],\n",
    "    momentum=hyperparameters[\"momentum\"],\n",
    "    weight_decay=hyperparameters[\"weight_decay\"],\n",
    "    warmup_epochs=hyperparameters[\"warmup_epochs\"],\n",
    "    warmup_momentum=hyperparameters[\"warmup_momentum\"],\n",
    "    warmup_bias_lr=hyperparameters[\"warmup_bias_lr\"],\n",
    "    hsv_h=hyperparameters[\"hsv_h\"],\n",
    "    hsv_s=hyperparameters[\"hsv_s\"],\n",
    "    hsv_v=hyperparameters[\"hsv_v\"],\n",
    "    degrees=hyperparameters[\"degrees\"],\n",
    "    translate=hyperparameters[\"translate\"],\n",
    "    scale=hyperparameters[\"scale\"],\n",
    "    shear=hyperparameters[\"shear\"],\n",
    "    perspective=hyperparameters[\"perspective\"],\n",
    "    flipud=hyperparameters[\"flipud\"],\n",
    "    fliplr=hyperparameters[\"fliplr\"],\n",
    "    mosaic=hyperparameters[\"mosaic\"],\n",
    "    mixup=hyperparameters[\"mixup\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.73 🚀 Python-3.9.7 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 4090, 24098MiB)\n",
      "YOLOv5s summary (fused): 193 layers, 9,113,858 parameters, 0 gradients, 23.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/mt23mcs002/Downloads/Augmented Images Python/Dataset/valid/labels.cache... 7129 images, 0 backgrounds, 0 corrupt: 100%|██████████| 7129/7129 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 446/446 [00:14<00:00, 30.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       7129       9877      0.965      0.951      0.982      0.876\n",
      "                Cracks       1296       1624      0.886      0.852      0.944      0.711\n",
      "         Edge_Settling       1283       1365       0.99      0.957      0.993      0.957\n",
      "               No_Road       1232       1232      0.991          1      0.995      0.994\n",
      "              Potholes       1409       1800      0.985      0.983      0.994      0.866\n",
      "              Raveling       1952       2502       0.95      0.913      0.971      0.774\n",
      "               Rutting       1315       1354      0.986          1      0.995      0.953\n",
      "Speed: 0.1ms preprocess, 1.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mroad_defect_yolo/experiment3\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "metrics = model.val(data=data_yaml_path)\n",
    "logging.info(f\"Validation metrics: {metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
